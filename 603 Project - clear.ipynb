{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sh\n",
    "pip install koalas\n",
    "pip install lxml\n",
    "pip install pymongo\n",
    "pip install pymongo[srv]\n",
    "pip install pymongo[tls]\n",
    "pip install dnspython\n",
    "pip install mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import databricks.koalas as ks\n",
    "import lxml\n",
    "from pymongo import MongoClient\n",
    "from pyspark.sql.types import *\n",
    "import pyspark.sql\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import OneHotEncoderEstimator, StringIndexer, VectorAssembler\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import mlflow\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MongoClient(\"mongodb+srv://isabelle1:data603@cluster0-t7ekj.mongodb.net/dataPI_db?retryWrites=true&w=majority\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = client.get_database('dataPI_db')\n",
    "\n",
    "info_p = db.performance\n",
    "info_p.count_documents({})\n",
    "\n",
    "pandas_df =  pd.DataFrame(list(info_p.find()))\n",
    "\n",
    "df = pandas_df.drop(['_id'], axis = 1)\n",
    "df_koalas = ks.DataFrame(df)\n",
    "df_koalas.head()\n",
    "\n",
    "# Here we have converted our target / label / y variable into an integer.\n",
    "df_koalas['Winner'] = df_koalas['Winner'].replace({'Home': 0, 'Away': 1}).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual Feature Reduction\n",
    "\n",
    "df = df_koalas[['D-1%', 'D-BO', 'D-CG', 'D-CM', 'D-IF', 'D-MI', 'F-BO', 'F-CM', 'F-FA', 'F-GA', 'F-KE', 'F-MI', 'F-RB', 'M-1%', 'M-GA', 'M-KE', 'M-RB', 'R-1%', 'R-BO', 'R-FF', 'R-GA', 'R-HB', 'R-HO', 'R-MI', 'R-RB', 'R-TK', 'R-UP', 'RLP', 'Same / Neutral Venue', 'Winner']]\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Machine Learning Section\n",
    "\n",
    "test = df.to_spark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "stages = []\n",
    "  \n",
    "label_stringIdx = StringIndexer(inputCol = 'Winner', outputCol = 'label')\n",
    "stages += [label_stringIdx]\n",
    "\n",
    "numericCols = ['D-1%', 'D-BO', 'D-CG', 'D-CM', 'D-IF', 'D-MI', 'F-BO', 'F-CM', 'F-FA', 'F-GA', 'F-KE', 'F-MI', 'F-RB', 'M-1%', 'M-GA', 'M-KE', 'M-RB', 'R-1%', 'R-BO', 'R-FF', 'R-GA', 'R-HB', 'R-HO', 'R-MI', 'R-RB', 'R-TK', 'R-UP', 'RLP']\n",
    "\n",
    "assemblerInputs = numericCols\n",
    "assembler = VectorAssembler(inputCols = assemblerInputs, outputCol = \"features\")\n",
    "stages += [assembler]\n",
    "\n",
    "# This is critical!\n",
    "\n",
    "dataset = assembler.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingData, testData = dataset.randomSplit([0.7, 0.3], seed = 100)\n",
    "print(trainingData.count())\n",
    "print(testData.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StandardScaler\n",
    "scaler = StandardScaler(withStd = True, withMean = True, inputCol = 'features', outputCol = 'scaled_features')\n",
    "\n",
    "scaler_model = scaler.fit(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_trainingData = scaler_model.transform(trainingData)\n",
    "scaled_testData = scaler_model.transform(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_trainingData = scaled_trainingData.withColumnRenamed('Winner', 'label')\n",
    "scaled_testData = scaled_testData.withColumnRenamed('Winner', 'label')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and Evaluate Models\n",
    "\n",
    "lr = LogisticRegression(labelCol = 'label', featuresCol = 'scaled_features', maxIter = 10)\n",
    "\n",
    "lrModel = lr.fit(scaled_trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = lrModel.transform(scaled_testData)\n",
    "\n",
    "# predictions1 = predictions.withColumnRenamed('Winner', 'label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected = predictions.select('label', 'prediction', 'probability')\n",
    "display(selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol = 'prediction')\n",
    "baseline_model = evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here I want to try a CrossGrid Validation to find better paramaters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "paramGrid = ParamGridBuilder()\\\n",
    ".addGrid(lr.aggregationDepth, [2,5,10])\\\n",
    ".addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0])\\\n",
    ".addGrid(lr.fitIntercept, [False, True])\\\n",
    ".addGrid(lr.regParam, [0.01, 0.5, 2.0])\\\n",
    ".build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 5-fold CrossValidator\n",
    "\n",
    "cv = CrossValidator(estimator = lr, estimatorParamMaps = paramGrid, evaluator = evaluator, numFolds = 5)\n",
    "\n",
    "# Run cross validations\n",
    "\n",
    "cvModel = cv.fit(scaled_trainingData)\n",
    "\n",
    "# This can take about 15 minutes to run.\n",
    "# There is a warning about mlflow logging, but it requires a library, which we have not loaded.  This is just a warning, which you can safely ignore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_train = cvModel.transform(scaled_trainingData)\n",
    "predict_test = cvModel.transform(scaled_testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The area under ROC for the training set after CV Grid is: {}'.format(evaluator.evaluate(predict_train)))\n",
    "print('The area under ROC for the test set after CV Grid is {}'.format(evaluator.evaluate(predict_test)))\n",
    "\n",
    "print('\\rThe area under ROC for the original (baseline) model is {}'.format(baseline_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# End of CrossGrid Validation section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ML REGRESSION ANALYSIS\n",
    "\n",
    "# Here we're going to keep the same features, but we are going to change the label to 'Net Score' instead of the binary classification problem of simply which team won."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_regression = df_koalas[['D-1%', 'D-BO', 'D-CG', 'D-CM', 'D-IF', 'D-MI', 'F-BO', 'F-CM', 'F-FA', 'F-GA', 'F-KE', 'F-MI', 'F-RB', 'M-1%', 'M-GA', 'M-KE', 'M-RB', 'R-1%', 'R-BO', 'R-FF', 'R-GA', 'R-HB', 'R-HO', 'R-MI', 'R-RB', 'R-TK', 'R-UP', 'RLP', 'Same / Neutral Venue', 'Net Score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_regression_df = df_regression.to_spark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "stages = []\n",
    "\n",
    "numericCols_regression = ['D-1%', 'D-BO', 'D-CG', 'D-CM', 'D-IF', 'D-MI', 'F-BO', 'F-CM', 'F-FA', 'F-GA', 'F-KE', 'F-MI', 'F-RB', 'M-1%', 'M-GA', 'M-KE', 'M-RB', 'R-1%', 'R-BO', 'R-FF', 'R-GA', 'R-HB', 'R-HO', 'R-MI', 'R-RB', 'R-TK', 'R-UP', 'RLP', 'Same / Neutral Venue']\n",
    "assemblerInputs_regression = numericCols_regression\n",
    "assembler_regression = VectorAssembler(inputCols = assemblerInputs_regression, outputCol = \"features\")\n",
    "\n",
    "regression_dataset = assembler.transform(spark_regression_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_trainingData, linear_testData = regression_dataset.randomSplit([0.7, 0.3], seed = 100)\n",
    "print(linear_trainingData.count())\n",
    "print(linear_testData.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StandardScaler\n",
    "scaler = StandardScaler(withStd = True, withMean = True, inputCol = 'features', outputCol = 'scaled_features')\n",
    "\n",
    "scaler_model = scaler.fit(linear_trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_linear_trainingData = scaler_model.transform(linear_trainingData)\n",
    "scaled_linear_testData = scaler_model.transform(linear_testData)\n",
    "scaled_linear_trainingData = scaled_linear_trainingData.withColumnRenamed('Net Score', 'label')\n",
    "scaled_linear_testData = scaled_linear_testData.withColumnRenamed('Net Score', 'label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see if this works\n",
    "algo = LinearRegression(featuresCol = 'scaled_features', labelCol = 'label')\n",
    "# algo = LinearRegression(featuresCol = 'features', labelCol = 'Net Score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = algo.fit(scaled_linear_trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_summary = model.evaluate(scaled_linear_testData)\n",
    "# evaluation_summary = model.evaluate(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Mean absolute error: \"+ str(evaluation_summary.meanAbsoluteError))\n",
    "print('Mean square root error: '+str(evaluation_summary.rootMeanSquaredError))\n",
    "print('R2: '+str(evaluation_summary.r2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.transform(scaled_linear_testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "name": "603 Project",
  "notebookId": 1304207961206366
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
